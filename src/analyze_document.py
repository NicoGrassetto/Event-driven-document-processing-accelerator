#!/usr/bin/env python3
"""
Azure AI Content Understanding Document Analyzer

This script analyzes documents using Azure AI Content Understanding service.
It can work with both prebuilt analyzers and custom schema analyzers.

Usage:
    python src/analyze_document.py [document_path]

Configuration:
    The script reads configuration from .analyzer-config file generated by create-analyzer.sh
    Or you can provide credentials manually when prompted.
"""

import os
import sys
import json
import time
import requests
import argparse
from typing import Dict, Any, Optional
from pathlib import Path


class DocumentAnalyzer:
    """Azure AI Content Understanding Document Analyzer"""
    
    def __init__(self, endpoint: str, api_key: str, analyzer_name: str):
        self.endpoint = endpoint.rstrip('/')
        self.api_key = api_key
        self.analyzer_name = analyzer_name
        self.analyzer_type = "extraction"  # Only custom analyzers supported
        
    def analyze_document(self, file_path: str) -> Dict[str, Any]:
        """
        Analyze a document using the configured analyzer
        
        Args:
            file_path: Path to the document file
            
        Returns:
            Dictionary containing analysis results
        """
        if not os.path.exists(file_path):
            raise FileNotFoundError(f"Document file not found: {file_path}")
        
        # Read document content
        with open(file_path, 'rb') as file:
            document_content = file.read()
        
        # Determine content type
        content_type = self._get_content_type(file_path)
        
        print(f"Analyzing document: {os.path.basename(file_path)}")
        print(f"Using analyzer: {self.analyzer_name} ({self.analyzer_type})")
        print(f"File size: {len(document_content):,} bytes")
        
        # Submit document for analysis
        operation_location = self._submit_document(document_content, content_type)
        
        # Poll for results
        results = self._poll_for_results(operation_location)
        
        # Process and return results
        return self._process_results(results, file_path)
    
    def _get_content_type(self, file_path: str) -> str:
        """Determine content type based on file extension"""
        extension = Path(file_path).suffix.lower()
        
        content_type_map = {
            '.pdf': 'application/pdf',
            '.jpg': 'image/jpeg',
            '.jpeg': 'image/jpeg',
            '.png': 'image/png',
            '.bmp': 'image/bmp',
            '.tiff': 'image/tiff',
            '.tif': 'image/tiff',
            '.gif': 'image/gif'
        }
        
        return content_type_map.get(extension, 'application/octet-stream')
    
    def _submit_document(self, document_content: bytes, content_type: str) -> str:
        """Submit document to the analyzer"""
        url = f"{self.endpoint}/contentunderstanding/analyzers/{self.analyzer_name}:analyze"
        
        headers = {
            "Content-Type": content_type,
            "Ocp-Apim-Subscription-Key": self.api_key
        }
        
        params = {"api-version": "2025-05-01-preview"}
        
        response = requests.post(url, headers=headers, params=params, data=document_content)
        
        if response.status_code != 202:
            raise Exception(f"Failed to submit document: {response.status_code} - {response.text}")
        
        operation_location = response.headers.get("Operation-Location")
        if not operation_location:
            raise Exception("No operation location returned from API")
        
        print("Document submitted successfully")
        return operation_location
    
    def _poll_for_results(self, operation_location: str) -> Dict[str, Any]:
        """Poll the operation location for results"""
        headers = {"Ocp-Apim-Subscription-Key": self.api_key}
        
        print("Waiting for analysis to complete", end="", flush=True)
        
        while True:
            response = requests.get(operation_location, headers=headers)
            
            if response.status_code != 200:
                raise Exception(f"Failed to get results: {response.status_code} - {response.text}")
            
            result = response.json()
            status = result.get("status", "unknown").lower()
            
            if status == "succeeded":
                print("\nAnalysis completed successfully!")
                # Return the result data
                if "result" in result:
                    return result["result"]
                elif "analyzeResult" in result:
                    return result["analyzeResult"]
                else:
                    return result
            elif status == "failed":
                error_message = result.get("error", {}).get("message", "Unknown error")
                raise Exception(f"Analysis failed: {error_message}")
            elif status in ["notstarted", "running"]:
                print(".", end="", flush=True)
                time.sleep(2)
            else:
                raise Exception(f"Unknown status: {status}")
    
    def _process_results(self, analysis_result: Dict[str, Any], file_path: str) -> Dict[str, Any]:
        """Process the analysis results into a structured format"""
        
        processed = {
            "metadata": {
                "filename": os.path.basename(file_path),
                "analyzer_name": self.analyzer_name,
                "analyzer_type": self.analyzer_type,
                "analysis_timestamp": time.strftime("%Y-%m-%d %H:%M:%S UTC", time.gmtime()),
                "api_version": "2025-05-01-preview"
            }
        }
        
        if self.analyzer_type == "extraction":
            # Process custom schema results
            processed.update(self._process_custom_extraction(analysis_result))
        else:
            # This shouldn't happen since we only support custom analyzers
            raise Exception("Unsupported analyzer type. Only custom schema analyzers are supported.")
        
        return processed
    
    def _process_custom_extraction(self, analysis_result: Dict[str, Any]) -> Dict[str, Any]:
        """Process results from custom schema analyzer"""
        
        processed = {
            "extraction_type": "custom_schema",
            "extracted_fields": {},
            "confidence_scores": {},
            "field_locations": {},
            "summary": {}
        }
        
        # Handle the new API structure
        if "contents" in analysis_result:
            contents = analysis_result["contents"]
            
            if contents and len(contents) > 0:
                content = contents[0]  # Process first content item
                
                # Check for fields structure (most common)
                if "fields" in content:
                    fields_data = content["fields"] 
                    for field_name, field_info in fields_data.items():
                        if isinstance(field_info, dict):
                            field_value = self._extract_field_value(field_info)
                            field_confidence = field_info.get("confidence", 0)
                            
                            processed["extracted_fields"][field_name] = field_value
                            processed["confidence_scores"][field_name] = field_confidence
                
                # Alternative: extractedFields structure
                elif "extractedFields" in content:
                    fields_data = content["extractedFields"]
                    
                    for field_name, field_info in fields_data.items():
                        if isinstance(field_info, dict):
                            field_value = field_info.get("value", "")
                            field_confidence = field_info.get("confidence", 0)
                            
                            processed["extracted_fields"][field_name] = field_value
                            processed["confidence_scores"][field_name] = field_confidence
        
        # Fallback: check if documents exist (older API structure)
        elif "documents" in analysis_result:
            documents = analysis_result.get("documents", [])
            
            if documents:
                document = documents[0]
                fields = document.get("fields", {})
                
                for field_name, field_data in fields.items():
                    if isinstance(field_data, dict):
                        field_value = self._extract_field_value(field_data)
                        field_confidence = field_data.get("confidence", 0)
                        
                        processed["extracted_fields"][field_name] = field_value
                        processed["confidence_scores"][field_name] = field_confidence
        
        # Calculate summary statistics
        confidence_values = list(processed["confidence_scores"].values())
        if confidence_values:
            processed["summary"] = {
                "total_fields_extracted": len(processed["extracted_fields"]),
                "fields_with_high_confidence": len([c for c in confidence_values if c > 0.8]),
                "fields_with_medium_confidence": len([c for c in confidence_values if 0.5 <= c <= 0.8]),
                "fields_with_low_confidence": len([c for c in confidence_values if c < 0.5]),
                "average_confidence": sum(confidence_values) / len(confidence_values),
                "extraction_quality": self._assess_extraction_quality(confidence_values)
            }
        else:
            processed["summary"] = {
                "total_fields_extracted": 0,
                "fields_with_high_confidence": 0,
                "fields_with_medium_confidence": 0,
                "fields_with_low_confidence": 0,
                "average_confidence": 0,
                "extraction_quality": "no_data"
            }
        
        return processed
    
    def _process_prebuilt_results(self, analysis_result: Dict[str, Any]) -> Dict[str, Any]:
        """Process results from prebuilt analyzer"""
        
        processed = {
            "extraction_type": "prebuilt_document",
            "pages": len(analysis_result.get("pages", [])),
            "extracted_text": "",
            "tables": [],
            "key_value_pairs": [],
            "summary": {}
        }
        
        # Extract text from all pages
        text_parts = []
        for page in analysis_result.get("pages", []):
            for line in page.get("lines", []):
                text_parts.append(line.get("content", ""))
        
        processed["extracted_text"] = "\n".join(text_parts)
        
        # Process tables
        for table in analysis_result.get("tables", []):
            table_data = {
                "row_count": table.get("rowCount", 0),
                "column_count": table.get("columnCount", 0),
                "cells": []
            }
            
            for cell in table.get("cells", []):
                table_data["cells"].append({
                    "row_index": cell.get("rowIndex"),
                    "column_index": cell.get("columnIndex"),
                    "content": cell.get("content", ""),
                    "kind": cell.get("kind", "content"),
                    "confidence": cell.get("confidence", 0)
                })
            
            processed["tables"].append(table_data)
        
        # Process key-value pairs
        for kv_pair in analysis_result.get("keyValuePairs", []):
            if kv_pair.get("key") and kv_pair.get("value"):
                processed["key_value_pairs"].append({
                    "key": kv_pair["key"].get("content", ""),
                    "value": kv_pair["value"].get("content", ""),
                    "confidence": kv_pair.get("confidence", 0)
                })
        
        # Create summary
        processed["summary"] = {
            "text_length": len(processed["extracted_text"]),
            "word_count": len(processed["extracted_text"].split()) if processed["extracted_text"] else 0,
            "table_count": len(processed["tables"]),
            "key_value_pairs_count": len(processed["key_value_pairs"]),
            "has_structured_data": len(processed["tables"]) > 0 or len(processed["key_value_pairs"]) > 0
        }
        
        return processed
    
    def _extract_field_value(self, field_data: Dict[str, Any]) -> Any:
        """Extract the appropriate value from field data"""
        
        field_type = field_data.get("type", "string")
        
        # Try different value keys based on type
        if "content" in field_data:
            return field_data["content"]
        elif "valueString" in field_data:
            return field_data["valueString"]
        elif "valueNumber" in field_data:
            return field_data["valueNumber"]
        elif "valueDate" in field_data:
            return field_data["valueDate"]
        elif "valueArray" in field_data:
            # Handle array fields
            array_items = []
            for item in field_data["valueArray"]:
                if isinstance(item, dict) and "valueObject" in item:
                    # Object array
                    obj_data = {}
                    for obj_field, obj_value in item["valueObject"].items():
                        obj_data[obj_field] = self._extract_field_value(obj_value)
                    array_items.append(obj_data)
                else:
                    array_items.append(self._extract_field_value(item))
            return array_items
        elif "valueObject" in field_data:
            # Handle object fields
            obj_data = {}
            for obj_field, obj_value in field_data["valueObject"].items():
                obj_data[obj_field] = self._extract_field_value(obj_value)
            return obj_data
        
        return None
    
    def _assess_extraction_quality(self, confidence_values: list) -> str:
        """Assess the overall quality of extraction"""
        if not confidence_values:
            return "no_data"
        
        avg_confidence = sum(confidence_values) / len(confidence_values)
        high_confidence_ratio = len([c for c in confidence_values if c > 0.8]) / len(confidence_values)
        
        if avg_confidence > 0.9 and high_confidence_ratio > 0.8:
            return "excellent"
        elif avg_confidence > 0.7 and high_confidence_ratio > 0.6:
            return "good"
        elif avg_confidence > 0.5:
            return "fair"
        else:
            return "poor"


def load_config() -> Optional[Dict[str, str]]:
    """Load configuration from .analyzer-config file"""
    # Look for config file in the parent directory (project root)
    config_file = os.path.join(os.path.dirname(os.path.dirname(__file__)), ".analyzer-config")
    
    if not os.path.exists(config_file):
        return None
    
    config = {}
    try:
        with open(config_file, 'r') as f:
            for line in f:
                line = line.strip()
                if line and not line.startswith('#') and '=' in line:
                    key, value = line.split('=', 1)
                    config[key.strip()] = value.strip()
        return config
    except Exception as e:
        print(f"Warning: Error reading config file: {e}")
        return None


def get_credentials_interactive() -> Dict[str, str]:
    """Get credentials interactively from user"""
    print("Configuration needed")
    print("Please provide your Azure AI Content Understanding service details:")
    
    endpoint = input("Enter your service endpoint: ").strip()
    api_key = input("Enter your API key: ").strip()
    analyzer_name = input("Enter analyzer name: ").strip()
    analyzer_type = "extraction"
    
    return {
        "CONTENT_UNDERSTANDING_ENDPOINT": endpoint,
        "CONTENT_UNDERSTANDING_KEY": api_key,
        "ANALYZER_NAME": analyzer_name,
        "ANALYZER_TYPE": analyzer_type
    }


def save_results(results: Dict[str, Any], output_file: Optional[str] = None) -> str:
    """Save results to JSON file"""
    if not output_file:
        timestamp = int(time.time())
        analyzer_name = results["metadata"]["analyzer_name"].replace("-", "_")
        output_file = f"analysis_{analyzer_name}_{timestamp}.json"
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    
    return output_file


def print_summary(results: Dict[str, Any]):
    """Print a summary of analysis results"""
    print("\n" + "="*60)
    print("ANALYSIS SUMMARY")
    print("="*60)
    
    metadata = results["metadata"]
    print(f"File: {metadata['filename']}")
    print(f"Analyzer: {metadata['analyzer_name']} ({metadata['analyzer_type']})")
    print(f"Analyzed: {metadata['analysis_timestamp']}")
    
    # Custom schema results only
    print(f"\nExtraction Type: Custom Schema")
    print(f"Fields Extracted: {results['summary']['total_fields_extracted']}")
    print(f"High Confidence: {results['summary']['fields_with_high_confidence']}")
    print(f"Medium Confidence: {results['summary']['fields_with_medium_confidence']}")
    print(f"Low Confidence: {results['summary']['fields_with_low_confidence']}")
    print(f"Average Confidence: {results['summary']['average_confidence']:.2%}")
    print(f"Quality Assessment: {results['summary']['extraction_quality'].title()}")
    
    print(f"\nExtracted Fields:")
    for field_name, field_value in results["extracted_fields"].items():
        confidence = results["confidence_scores"][field_name]
        confidence_icon = "[HIGH]" if confidence > 0.8 else "[MED]" if confidence > 0.5 else "[LOW]"
        print(f"  {confidence_icon} {field_name}: {field_value} ({confidence:.1%})")


def main():
    parser = argparse.ArgumentParser(description="Analyze documents with Azure AI Content Understanding")
    parser.add_argument("document", nargs="?", help="Path to document file to analyze")
    parser.add_argument("-o", "--output", help="Output file path for results")
    parser.add_argument("--config", help="Path to configuration file")
    
    args = parser.parse_args()
    
    # Get document path
    document_path = args.document
    if not document_path:
        document_path = input("Enter path to document file: ").strip()
    
    if not document_path or not os.path.exists(document_path):
        print(f"Error: Document file not found: {document_path}")
        sys.exit(1)
    
    # Load configuration
    print("Loading configuration...")
    config = load_config()
    
    if not config:
        print("No configuration file found (.analyzer-config)")
        config = get_credentials_interactive()
    else:
        print("Configuration loaded from .analyzer-config")
    
    # Validate configuration
    required_keys = ["CONTENT_UNDERSTANDING_ENDPOINT", "CONTENT_UNDERSTANDING_KEY", "ANALYZER_NAME"]
    missing_keys = [key for key in required_keys if not config.get(key)]
    
    if missing_keys:
        print(f"Error: Missing configuration keys: {', '.join(missing_keys)}")
        sys.exit(1)
    
    try:
        # Create analyzer instance
        analyzer = DocumentAnalyzer(
            endpoint=config["CONTENT_UNDERSTANDING_ENDPOINT"],
            api_key=config["CONTENT_UNDERSTANDING_KEY"],
            analyzer_name=config["ANALYZER_NAME"]
        )
        
        # Analyze document
        results = analyzer.analyze_document(document_path)
        
        # Print summary
        print_summary(results)
        
        # Save results
        output_file = save_results(results, args.output)
        print(f"\nResults saved to: {output_file}")
        
        print("\nAnalysis completed successfully!")
        
    except Exception as e:
        print(f"\nError during analysis: {str(e)}")
        sys.exit(1)


if __name__ == "__main__":
    main()