# Azure AI Content Understanding Document Analyzer

This project provides a streamlined solution for analyzing documents using Azure AI Content Understanding service with custom schema analyzers.

## üèóÔ∏è Project Structure

```
‚îú‚îÄ‚îÄ infra/                      # Azure infrastructure
‚îÇ   ‚îú‚îÄ‚îÄ main.bicep             # Bicep template for AI Content Understanding service
‚îÇ   ‚îî‚îÄ‚îÄ modules/               # Reusable Bicep modules
‚îú‚îÄ‚îÄ schemas/
‚îÇ   ‚îî‚îÄ‚îÄ schema.json            # Your custom extraction schema
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îî‚îÄ‚îÄ analyze_document.py    # Python script for document analysis
‚îú‚îÄ‚îÄ deploy.sh                  # Deploy Azure infrastructure
‚îú‚îÄ‚îÄ create-analyzer.sh         # Create custom analyzer from schema.json
‚îú‚îÄ‚îÄ requirements.txt           # Python dependencies
‚îî‚îÄ‚îÄ .analyzer-config           # Auto-generated configuration (git-ignored)
```

## üöÄ Quick Start

### 1. Prerequisites

- Azure CLI installed and logged in (`az login`)
- Python 3.7+ installed
- `jq` installed (optional, for better JSON formatting): `brew install jq`

### 2. Deploy Infrastructure

```bash
# Deploy the AI Content Understanding service to Azure
./deploy.sh
```

### 3. Create Your Custom Schema

Edit `schemas/schema.json` to define what you want to extract:

```json
{
  "version": "1.0",
  "description": "Custom document analyzer schema",
  "fields": [
    {
      "fieldKey": "InvoiceNumber",
      "fieldType": "string",
      "description": "Invoice number"
    },
    {
      "fieldKey": "TotalAmount",
      "fieldType": "number",
      "fieldFormat": "currency",
      "description": "Total amount"
    }
  ]
}
```

### 4. Create the Analyzer

```bash
# Creates analyzer from schemas/schema.json
./create-analyzer.sh
```

This will:
- Read your custom schema from `schemas/schema.json`
- Create the analyzer in Azure
- Generate `.analyzer-config` with credentials

### 5. Analyze Documents

```bash
# Analyze a document
python src/analyze_document.py path/to/document.pdf
```

## üìã What is .analyzer-config?

The `.analyzer-config` file is automatically generated by `create-analyzer.sh` and stores your Azure credentials:

```bash
# Azure AI Content Understanding Configuration
CONTENT_UNDERSTANDING_ENDPOINT=https://your-service.cognitiveservices.azure.com/
CONTENT_UNDERSTANDING_KEY=your-api-key
ANALYZER_NAME=custom-schema-analyzer
ANALYZER_TYPE=extraction
```

**Purpose:**
- Stores Azure credentials securely
- Links shell script and Python script
- Enables automation without prompts
- Git-ignored for security

## üéØ Custom Schema Format

### Field Types
- `string`: Text content
- `number`: Numeric values  
- `date`: Date values
- `array`: Lists of items
- `object`: Complex structured data
- `address`: Address information

### Field Formats
- `alphanumeric`: Letters and numbers
- `currency`: Monetary amounts
- `percentage`: Percentage values
- `dmy`: Day-Month-Year dates

### Example Schemas

**Invoice Schema:**
```json
{
  "version": "1.0",
  "description": "Invoice extraction schema",
  "fields": [
    {
      "fieldKey": "InvoiceNumber",
      "fieldType": "string",
      "fieldFormat": "alphanumeric"
    },
    {
      "fieldKey": "InvoiceDate",
      "fieldType": "date",
      "fieldFormat": "dmy"
    },
    {
      "fieldKey": "VendorName",
      "fieldType": "string"
    },
    {
      "fieldKey": "TotalAmount",
      "fieldType": "number",
      "fieldFormat": "currency"
    }
  ]
}
```

**Contract Schema:**
```json
{
  "version": "1.0", 
  "description": "Contract extraction schema",
  "fields": [
    {
      "fieldKey": "ContractNumber",
      "fieldType": "string"
    },
    {
      "fieldKey": "PartyA",
      "fieldType": "string"
    },
    {
      "fieldKey": "PartyB", 
      "fieldType": "string"
    },
    {
      "fieldKey": "ContractValue",
      "fieldType": "number",
      "fieldFormat": "currency"
    },
    {
      "fieldKey": "StartDate",
      "fieldType": "date"
    }
  ]
}
```

## üìä Analysis Output

```json
{
  "metadata": {
    "filename": "invoice.pdf",
    "analyzer_name": "custom-schema-analyzer",
    "analyzer_type": "extraction"
  },
  "extraction_type": "custom_schema",
  "extracted_fields": {
    "InvoiceNumber": "INV-2023-001",
    "TotalAmount": 1500.00,
    "VendorName": "ACME Corp"
  },
  "confidence_scores": {
    "InvoiceNumber": 0.99,
    "TotalAmount": 0.95,
    "VendorName": 0.98
  },
  "summary": {
    "total_fields_extracted": 3,
    "fields_with_high_confidence": 3,
    "average_confidence": 0.97,
    "extraction_quality": "excellent"
  }
}
```

## üîß Configuration

### Manual Configuration

Create `.analyzer-config` manually if needed:

```bash
CONTENT_UNDERSTANDING_ENDPOINT=https://your-service.cognitiveservices.azure.com/
CONTENT_UNDERSTANDING_KEY=your-api-key
ANALYZER_NAME=custom-schema-analyzer
ANALYZER_TYPE=extraction
```

## üìù Supported File Types

- **PDF**: `.pdf`
- **Images**: `.jpg`, `.jpeg`, `.png`, `.bmp`, `.tiff`, `.tif`, `.gif`

## üõ†Ô∏è Advanced Usage

### Programmatic Usage

```python
from analyze_document import DocumentAnalyzer
import os

# Load from config file or set manually
analyzer = DocumentAnalyzer(
    endpoint=os.environ["CONTENT_UNDERSTANDING_ENDPOINT"],
    api_key=os.environ["CONTENT_UNDERSTANDING_KEY"],
    analyzer_name=os.environ["ANALYZER_NAME"]
)

# Analyze document
results = analyzer.analyze_document("document.pdf")
print(f"Extracted: {results['extracted_fields']}")
```

## üîí Security

- **Credentials**: `.analyzer-config` is git-ignored
- **API Keys**: Rotate regularly in Azure Portal
- **Access Control**: Use Azure RBAC
- **Network**: Consider private endpoints for production

## üìã Custom Schemas

### Creating Custom Schemas

Custom schemas are defined in JSON files in the `schemas/` directory. Each schema defines the fields you want to extract from documents.

#### Schema Structure

```json
{
  "version": "1.0",
  "description": "Description of what this schema extracts",
  "fields": [
    {
      "fieldKey": "FieldName",
      "fieldType": "string|number|date|array|object|address",
      "fieldFormat": "alphanumeric|currency|percentage|dmy",
      "description": "What this field represents"
    }
  ]
}
```

#### Field Types

- **string**: Text content
- **number**: Numeric values
- **date**: Date values
- **array**: Lists of items
- **object**: Complex structured data
- **address**: Address information

#### Field Formats

- **alphanumeric**: Letters and numbers
- **currency**: Monetary amounts
- **percentage**: Percentage values
- **dmy**: Day-Month-Year date format

#### Example: Custom Business Card Schema

```json
{
  "version": "1.0",
  "description": "Extract contact information from business cards",
  "fields": [
    {
      "fieldKey": "PersonName",
      "fieldType": "string",
      "description": "Full name of the person"
    },
    {
      "fieldKey": "JobTitle",
      "fieldType": "string",
      "description": "Job title or position"
    },
    {
      "fieldKey": "Company",
      "fieldType": "string",
      "description": "Company name"
    },
    {
      "fieldKey": "Email",
      "fieldType": "string",
      "description": "Email address"
    },
    {
      "fieldKey": "Phone",
      "fieldType": "string",
      "description": "Phone number"
    },
    {
      "fieldKey": "Address",
      "fieldType": "address",
      "description": "Business address"
    }
  ]
}
```

Save this as `schemas/business-card-schema.json` and it will appear as an option when running `./create-analyzer.sh`.

## üéØ Analysis Results

### Prebuilt Analyzer Output

```json
{
  "metadata": {
    "filename": "document.pdf",
    "analyzer_name": "prebuilt-document-analyzer",
    "analyzer_type": "prebuilt-document"
  },
  "extraction_type": "prebuilt_document",
  "pages": 2,
  "extracted_text": "Full document text...",
  "tables": [...],
  "key_value_pairs": [...],
  "summary": {
    "text_length": 1500,
    "word_count": 250,
    "table_count": 1,
    "key_value_pairs_count": 5
  }
}
```

### Custom Schema Output

```json
{
  "metadata": {
    "filename": "invoice.pdf",
    "analyzer_name": "custom-invoice-analyzer",
    "analyzer_type": "extraction"
  },
  "extraction_type": "custom_schema",
  "extracted_fields": {
    "InvoiceNumber": "INV-2023-001",
    "InvoiceDate": "2023-12-01",
    "VendorName": "ACME Corp",
    "TotalAmount": 1500.00
  },
  "confidence_scores": {
    "InvoiceNumber": 0.99,
    "InvoiceDate": 0.95,
    "VendorName": 0.98,
    "TotalAmount": 0.92
  },
  "summary": {
    "total_fields_extracted": 4,
    "fields_with_high_confidence": 4,
    "average_confidence": 0.96,
    "extraction_quality": "excellent"
  }
}
```

## üîß Configuration

### Environment Variables

The `create-analyzer.sh` script generates a `.analyzer-config` file with:

```bash
CONTENT_UNDERSTANDING_ENDPOINT=https://your-service.cognitiveservices.azure.com/
CONTENT_UNDERSTANDING_KEY=your-api-key
ANALYZER_NAME=your-analyzer-name
ANALYZER_TYPE=prebuilt-document|extraction
```

### Manual Configuration

If you need to configure manually, create `.analyzer-config`:

```bash
# Azure AI Content Understanding Configuration
CONTENT_UNDERSTANDING_ENDPOINT=https://your-service.cognitiveservices.azure.com/
CONTENT_UNDERSTANDING_KEY=your-32-character-api-key
ANALYZER_NAME=my-analyzer-name
ANALYZER_TYPE=prebuilt-document
```

## üìù Supported File Types

- **PDF**: `.pdf`
- **Images**: `.jpg`, `.jpeg`, `.png`, `.bmp`, `.tiff`, `.tif`, `.gif`

## üõ†Ô∏è Advanced Usage

### Batch Processing

```python
from analyze_document import DocumentAnalyzer

# Initialize analyzer
analyzer = DocumentAnalyzer(endpoint, api_key, analyzer_name, analyzer_type)

# Process multiple documents
documents = ["doc1.pdf", "doc2.pdf", "doc3.pdf"]
results = []

for doc in documents:
    result = analyzer.analyze_document(doc)
    results.append(result)
    print(f"Processed: {doc}")
```

### Integration with Azure Functions

Use the `DocumentAnalyzer` class in your Azure Functions:

```python
import azure.functions as func
from analyze_document import DocumentAnalyzer

def main(myblob: func.InputStream) -> None:
    # Initialize analyzer from environment variables
    analyzer = DocumentAnalyzer(
        endpoint=os.environ["CONTENT_UNDERSTANDING_ENDPOINT"],
        api_key=os.environ["CONTENT_UNDERSTANDING_KEY"],
        analyzer_name=os.environ["ANALYZER_NAME"],
        analyzer_type=os.environ.get("ANALYZER_TYPE", "prebuilt-document")
    )
    
    # Process the blob content
    blob_content = myblob.read()
    
    # Save to temporary file and analyze
    with tempfile.NamedTemporaryFile(suffix='.pdf') as tmp_file:
        tmp_file.write(blob_content)
        tmp_file.flush()
        
        results = analyzer.analyze_document(tmp_file.name)
        # Process results as needed
```

## üîí Security

- **Credentials**: Never commit `.analyzer-config` to version control
- **API Keys**: Rotate API keys regularly
- **Access Control**: Use Azure RBAC to control access to the service
- **Network Security**: Consider using private endpoints for production

## üêõ Troubleshooting

### Common Issues

1. **"analyzer not found"**: Make sure you created the analyzer with `create-analyzer.sh`
2. **"authentication failed"**: Check your endpoint and API key in `.analyzer-config`
3. **"unsupported file type"**: Verify your file is PDF or a supported image format
4. **"low confidence scores"**: Try using a custom schema tailored to your document type

### Debug Mode

Add debug logging to the Python script:

```python
import logging
logging.basicConfig(level=logging.DEBUG)
```

### API Rate Limits

The service has rate limits. If you get rate limit errors:
- Add delays between requests
- Use exponential backoff
- Consider upgrading your service tier

## üìö Learn More

- [Azure AI Content Understanding Documentation](https://docs.microsoft.com/en-us/azure/ai-services/content-understanding/)
- [Custom Schema Reference](https://docs.microsoft.com/en-us/azure/ai-services/content-understanding/concept-custom)
- [API Reference](https://docs.microsoft.com/en-us/rest/api/contentunderstanding/)

## ü§ù Contributing

1. Fork the repository
2. Create a feature branch
3. Add new schemas to `schemas/` directory
4. Test your changes
5. Submit a pull request

## üìÑ License

This project is licensed under the MIT License.